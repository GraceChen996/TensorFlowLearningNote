{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGQKf+gzKuYD4pzRAVBf+l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GraceChen996/TensorFlowLearningNote/blob/main/example2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0pkjrowvRGx",
        "outputId": "0aca4085-b1d2-4845-9354-c710dca651dc"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (5.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (4.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense "
      ],
      "metadata": {
        "id": "cvMD15wsukxO"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class PolicyGradientNetwork(keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims=256, fc2_dims=256):\n",
        "        super(PolicyGradientNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.pi = Dense(n_actions, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        value = self.fc1(state)\n",
        "        value = self.fc2(value)\n",
        "\n",
        "        pi = self.pi(value)\n",
        "\n",
        "        return pi\n"
      ],
      "metadata": {
        "id": "rvw_1ilEuj4W"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, alpha=0.003, gamma=0.99, n_actions=4,\n",
        "                 layer1_size=256, layer2_size=256):\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.lr = alpha\n",
        "        self.n_actions = n_actions\n",
        "        self.state_memory = []\n",
        "        self.action_memory = []\n",
        "        self.reward_memory = []\n",
        "        self.policy = PolicyGradientNetwork(n_actions=n_actions)\n",
        "        self.policy.compile(optimizer=Adam(learning_rate=self.lr))\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
        "        probs = self.policy(state)\n",
        "        action_probs = tfp.distributions.Categorical(probs=probs)\n",
        "        action = action_probs.sample()\n",
        "\n",
        "        return action.numpy()[0]\n",
        "\n",
        "    def store_transition(self, observation, action, reward):\n",
        "        self.state_memory.append(observation)\n",
        "        self.action_memory.append(action)\n",
        "        self.reward_memory.append(reward)\n",
        "\n",
        "    def learn(self):\n",
        "        actions = tf.convert_to_tensor(self.action_memory, dtype=tf.float32)\n",
        "        rewards = np.array(self.reward_memory)\n",
        "\n",
        "        G = np.zeros_like(rewards)\n",
        "        for t in range(len(rewards)):\n",
        "            G_sum = 0\n",
        "            discount = 1\n",
        "            for k in range(t, len(rewards)):\n",
        "                G_sum += rewards[k] * discount\n",
        "                discount *= self.gamma\n",
        "            G[t] = G_sum\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = 0\n",
        "            for idx, (g, state) in enumerate(zip(G, self.state_memory)):\n",
        "                state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "                probs = self.policy(state)\n",
        "                action_probs = tfp.distributions.Categorical(probs=probs)\n",
        "                log_prob = action_probs.log_prob(actions[idx])\n",
        "                loss += -g * tf.squeeze(log_prob)\n",
        "\n",
        "        gradient = tape.gradient(loss, self.policy.trainable_variables)\n",
        "        self.policy.optimizer.apply_gradients(zip(gradient, self.policy.trainable_variables))\n",
        "\n",
        "        self.state_memory = []\n",
        "        self.action_memory = []\n",
        "        self.reward_memory = []"
      ],
      "metadata": {
        "id": "k1o2-ANsvE-O"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    agent = Agent(alpha=0.0005,  gamma=0.99,n_actions=4)\n",
        "\n",
        "    env = gym.make('LunarLander-v2')\n",
        "    score_history = []\n",
        "\n",
        "    num_episodes = 50\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        done = False\n",
        "        score = 0\n",
        "        observation = env.reset()\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            agent.store_transition(observation, action, reward)\n",
        "            observation = observation_\n",
        "            score += reward\n",
        "        score_history.append(score)\n",
        "\n",
        "        agent.learn()\n",
        "        avg_score = np.mean(score_history[-100:])\n",
        "        print('episode: ', i,'score: %.1f' % score,\n",
        "            'average score %.1f' % avg_score)\n",
        "\n",
        "    filename = 'lunar-lander.png'\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt-LYGL5ueSe",
        "outputId": "cfe6001e-f592-467b-e2a2-b97db6dca52f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode:  0 score: -153.7 average score -153.7\n",
            "episode:  1 score: -206.3 average score -180.0\n",
            "episode:  2 score: -247.2 average score -202.4\n",
            "episode:  3 score: -224.4 average score -207.9\n",
            "episode:  4 score: -223.4 average score -211.0\n",
            "episode:  5 score: -106.9 average score -193.6\n",
            "episode:  6 score: -516.6 average score -239.8\n",
            "episode:  7 score: -322.3 average score -250.1\n",
            "episode:  8 score: -254.3 average score -250.6\n",
            "episode:  9 score: -126.4 average score -238.1\n",
            "episode:  10 score: -92.7 average score -224.9\n",
            "episode:  11 score: -150.3 average score -218.7\n",
            "episode:  12 score: -258.1 average score -221.7\n",
            "episode:  13 score: -60.0 average score -210.2\n",
            "episode:  14 score: -78.2 average score -201.4\n",
            "episode:  15 score: -119.1 average score -196.2\n",
            "episode:  16 score: -247.0 average score -199.2\n",
            "episode:  17 score: -16.7 average score -189.1\n",
            "episode:  18 score: -48.8 average score -181.7\n",
            "episode:  19 score: -35.6 average score -174.4\n",
            "episode:  20 score: -70.1 average score -169.4\n",
            "episode:  21 score: -129.8 average score -167.6\n",
            "episode:  22 score: -124.2 average score -165.7\n",
            "episode:  23 score: -34.7 average score -160.3\n",
            "episode:  24 score: -149.4 average score -159.8\n",
            "episode:  25 score: -331.1 average score -166.4\n",
            "episode:  26 score: -229.9 average score -168.8\n",
            "episode:  27 score: -128.8 average score -167.4\n",
            "episode:  28 score: -134.4 average score -166.2\n",
            "episode:  29 score: -141.1 average score -165.4\n",
            "episode:  30 score: -197.0 average score -166.4\n",
            "episode:  31 score: -341.8 average score -171.9\n",
            "episode:  32 score: -139.3 average score -170.9\n",
            "episode:  33 score: -368.9 average score -176.7\n",
            "episode:  34 score: -218.1 average score -177.9\n",
            "episode:  35 score: -188.8 average score -178.2\n",
            "episode:  36 score: -314.6 average score -181.9\n",
            "episode:  37 score: -510.2 average score -190.5\n",
            "episode:  38 score: -229.2 average score -191.5\n",
            "episode:  39 score: -21.8 average score -187.3\n",
            "episode:  40 score: -93.7 average score -185.0\n",
            "episode:  41 score: -132.9 average score -183.8\n",
            "episode:  42 score: -489.7 average score -190.9\n",
            "episode:  43 score: -140.6 average score -189.7\n",
            "episode:  44 score: -183.2 average score -189.6\n",
            "episode:  45 score: -59.2 average score -186.7\n",
            "episode:  46 score: -76.6 average score -184.4\n",
            "episode:  47 score: -118.5 average score -183.0\n",
            "episode:  48 score: -163.1 average score -182.6\n",
            "episode:  49 score: -97.2 average score -180.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plotLearning(score_history, filename=filename, window=100)"
      ],
      "metadata": {
        "id": "tJh6s1srzsRx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}